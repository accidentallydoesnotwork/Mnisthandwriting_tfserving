{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying ML model using tensorflow serving "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Prepare your saved model \n",
    "\n",
    "A SavedModel represents a version of your model. It is stored as a directory containing a saved_model.pb file, which defines the computation graph (represented as a serialized protocol buffer), and a variables subdirectory containing the variable values. Here is the code to save your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume that you have already train your model\n",
    "model_version = \"0001\"\n",
    "model_name = \"my_mnist_model\"\n",
    "model_path = os.path.join(model_name, model_version)\n",
    "tf.saved_model.save(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory structure of your saved model is as follows: \n",
    "```\n",
    "my_mnist_model\n",
    "└── 0001\n",
    "    ├── assets\n",
    "    ├── saved_model.pb\n",
    "    └── variables\n",
    "        ├── variables.data-00000-of-00001\n",
    "        └── variables.index\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to load the model in your python file:\n",
    "\n",
    "you can load a SavedModel using the tf.saved_model.load() function. However, the returned object is not a Keras model: it represents the SavedModel, including its computation graph and variable values. You can use it like a function, and it will make predictions (make sure to pass the inputs as tensors of the appropriate type):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = tf.saved_model.load(model_path)\n",
    "y_pred = saved_model(tf.constant(X_new, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can load this SavedModel directly to a Keras model using the keras.models.load_model() function \n",
    "However, we only use load_model to test our saved model to see whether it is the correct model. Our goal is to deploy the model and send prediction request to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load using keras backend\n",
    "model = keras.models.load_model(model_path)\n",
    "y_pred = model.predict(tf.constant(X_new, dtype=tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) INSTALLING TENSORFLOW SERVING\n",
    "\n",
    "There are many ways to install TF Serving, but Docker is highly recommended by Tensorflow team due to its high performance and simple instalation. The guide below assume that you have already install docker. The following code is run on **terminal**\n",
    "\n",
    "First pull the TF Serving image: \n",
    "```\n",
    "docker pull tensorflow/serving\n",
    "```\n",
    "\n",
    "Create Docker container to run this image:\n",
    "```\n",
    "$ docker run -it --rm -p 8500:8500 -p 8501:8501 \\\n",
    "             -v \"$ML_PATH/my_mnist_model:/models/my_mnist_model\" \\\n",
    "             -e MODEL_NAME=my_mnist_model \\\n",
    "             tensorflow/serving\n",
    "```\n",
    "\n",
    "TF Serving is running. It loaded our MNIST model (version 1), and it is serving it through both gRPC (on port 8500) and REST (on port 8501). Here is the meaning of all Docker flags above:\n",
    "\n",
    "-it: interactive, which mean you can terminate the container using Ctrl-C\n",
    "\n",
    "--rm: remove the container after you stop it\n",
    "-p 8500:8500: Forward host TCP port 8500 to container's TCP port 8500. TF serving serve gRPC API through port 8500 (default)\n",
    "\n",
    "-p 8501:8501: Forward host TCP port 8501 to container's TCP port 8501. TF serving serve REST API through port 8501 (default)\n",
    "\n",
    "-v `host_path:container_path` : mount the host directory to container. In Windows, you may need to replace / with \\ in the host path.\n",
    "\n",
    "-e MODEL_NAME=my_mnist_model: Set the container's MODEL_NAME environment variable, so that TF Serving knows which model to serve. By default it serve the latest model it finds \n",
    "\n",
    "tensorflow/serving: name of the image to run \n",
    "\n",
    "Now your tensorflow model is running in a container image. In order to make predictions, we will send request to the REST API at 8501 port in the container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Send request through the REST API:\n",
    "\n",
    "Prepare JSON file to send request to TF Serving. JSON is puretext base so the input needs to be converted into python list \n",
    "\n",
    "Note: Predict SignatureDef (\"signature_name\": \"serving_default\")\n",
    "\n",
    "Predict SignatureDefs support calls to TensorFlow Serving's Predict API. These signatures allow you to flexibly support arbitrarily many input and output Tensors\n",
    "\n",
    "Pros and cons of using JSON\n",
    "Pros:\n",
    "- Easy to use\n",
    "- Does not require complicated dependencies \n",
    "Cons: \n",
    "- Need to transform to list, which is not very efficient\n",
    "- Convert float to string is not very ideal, especially for large data \n",
    "\n",
    "Solution? gRPC (google Remote Procedure Calls). This is an open source technology which is developed by google. It is a bit advance so we will not teach it in this class. Just understand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# X_new is our input data\n",
    "input_data_json = json.dumps({\n",
    "    \"signature_name\": \"serving_default\",\n",
    "    \"instances\": X_new.tolist(),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then send the input data by sending an HTTP POST request to TF Serving. This can be done by the request python library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "SERVER_URL = 'http://localhost:8501/v1/models/my_mnist_model:predict'\n",
    "response = requests.post(SERVER_URL, data=input_data_json)\n",
    "response.raise_for_status() # raise an exception in case of error\n",
    "response = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF Serving then return us with dictionary contain a single \"predictions\" key, which has the array of probabilities. You can get the \n",
    "y_proba = np.array(response[\"predictions\"])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
